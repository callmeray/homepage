<html>
<head>
	<meta  http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=1" />
	<link rel="stylesheet" type="text/css" href="utils/bootstrap.min.css"/>
  <script language="javascript" src="utils/jquery.min.js"></script>
	<script language="javascript" src="utils/bootstrap.min.js"></script>
  <!-- -->
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css"/>
	<link rel="stylesheet" type="text/css" href="utils/cssReset.css"/>
  <link rel="stylesheet" type="text/css" href="utils/css_layout.css"/>
	<title> Rui Chen/ 陈睿</title>
  <!-- 搜索引擎优化stuff -->
	<meta name="description"
    content="Academic website for Rui Chen. Dr. Rui Chen is a Research Assistant Professor at Dept. of Mechanical Engineering, Tsinghua University.">
	<meta name="keywords" content="Rui Chen, THU, Tsinghua University, Tactile Sensing, Robotics">


</head>
<body>
<style type="text/css">
  #main{
    background: #f7f7f7;
  }
  #main > div.publications > ol > li{
    background: white;
    box-shadow: 2px 5px 5px #c9c9c9;
    margin-bottom: 10px;
    padding: 20px;
  }
  #main div.img{
    padding: 20px;
    text-align: center;
    padding-right: 150px;
  }
  #main div.img > div.img_caption{
    font-weight: 450;
    font-size: 1.1em;
    line-height: 40px;
  }
</style>
<div id="sidebar">
  <img class='me' src="resources/me.webp"></img>
  <br/>
  <div class="info">
    <h2 class="name">Rui Chen</h2>
    <h2 class="name_chinese">陈睿</h2>
    <h2 class="email">chenruithu AT mail.tsinghua.edu.cn</h2>
    <h2 class="link">
      <a style="font-size: 14px; color: yellow; font-weight: bold;" href="https://scholar.google.com/citations?user=azTXasgAAAAJ">Google Scholar</a>
    </h2>


    <!--
    <a class="quickLink" href="https://medium.com/@junweil">
      <img class='medium' style="" src="resources/medium.png"></img>
    </a>
    <a class="quickLink" href="https://dblp.org/pers/hd/l/Liang_0001:Junwei">
      <img class='dblp' style="height:20px" src="resources/dblp.png"></img>
    </a>
    <a class="quickLink" href="http://aminer.cn/profile/junwei-liang/562cb48c45cedb3398c9e13b">
      <img class='aminer' style="height:20px;width: 50px;margin-top:4px" src="resources/aminer.png"></img>
    </a>
    <a class="quickLink" href="https://g.co/kgs/gTWf5W">
      <img class='aminer' name="Google knowledge graph" style="height:30px;width: 30px;margin-top:0px" src="resources/gkg.png"></img>
    </a>-->

  </div>
  <div id="navigation">
    <a class="nav_item" href="./index.html">
      <i class="icon icon-home icon-white"></i> &nbsp; About
    </a>
    <a class="nav_item" href="./index.html#events">
      <i class="icon icon-th-large icon-white"></i> &nbsp; Events
    </a>
    <a class="nav_item" href="./index.html#pubs">
      <i class="icon icon-file icon-white"></i> &nbsp; Publications
    </a>
    <a class="nav_item" href="./index.html#awards">
      <i class="icon icon-bookmark icon-white"></i> &nbsp; Awards
    </a>

  </div>
</div>

<div id="main">
  <div class="title">
    <a class="title_link" id="bio" href="#bio">About</a>
    <img id="logo" src="resources/thu_logo.png"></img>
  </div>
  <div class="content">
    I am a Research Assistant Professor at Dept. of Mechanical Engineering, Tsinghua University. My goal is to empower robots to better perceive,&nbsp;understand and interact with the world.&nbsp; My current research includes robot learning, tactile sensing, and 3D computer vision.

    <div class="linebreak"></div>
    I am actively looking for motivated visiting students, interns, and postdocs. Please feel free to email me if you are interested.

    <div class="linebreak"></div>
  </div>

  <div class="title">
    <a class="title_link" id="news" href="#news">News</a>
  </div>

  <div class="content">
    <ul>
      <li><span class="label label-info">01/2025</span>
        <a href="https://ieeexplore.ieee.org/document/10842357">ThinTact</a>  is accepted to TRO.
        </li>
      <li><span class="label label-info">10/2024</span>
      The webpage of ManiSkill-ViTac Challenge 2025 is online! Please check <a href="https://ai-workshops.github.io/maniskill-vitac-challenge-2025/" target="_blank">here</a> for more details.
      </li>
      <li>
        <span class="label label-info">03/2024</span>
        One paper accepted to CVPR 2024
      </li>
      <li>
        <span class="label label-info">01/2024</span>
        The ManiSkill-ViTac Challenge 2024 is released. Please check <a href="https://ai-workshops.github.io/maniskill-vitac-challenge-2024/"> here</a> for more details.
      </li>
      <li>
        <span class="label label-info">01/2024</span>
        Our paper <a href="https://ieeexplore.ieee.org/abstract/document/10388459" >''General-Purpose Sim2Real Protocol for Learning Contact-Rich Manipulation With Marker-Based Visuotactile Sensors''</a> is accepted to TRO.
      </li>
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="events" href="#events">Events / Workshops</a>
  </div>

  <div class="content">
    <ul>
      <li><a href="https://ai-workshops.github.io/maniskill-vitac-challenge-2025/" target="_blank">ManiSkill-ViTac: Tactile-Vision Manipulation Skill Learning Challenge 2025</a> </li>
      <li><a href="https://ai-workshops.github.io/maniskill-vitac-challenge-2024/" target="_blank">ManiSkill-ViTac: Vision-based-Tactile Manipulation Skill Learning Challenge 2024</a> </li>
      <li><a href="https://sapien.ucsd.edu/challenges/maniskill/" target="_blank">ManiSkill2 Challenge</a> </li>
      <li><a href="https://shanluo.github.io/ViTacWorkshops/vitac2024" target="_blank">ICRA 2024 Workshop on ''Robot Embodiment through Visuo-Tactile Perception''</a> </li>
      <li><a href="https://ai-workshops.github.io/interdisciplinary-exploration-of-gmpl/" target="_blank">RSS 2023 Workshop on ''Interdisciplinary Exploration of Generalizable Manipulation Policy Learning: Paradigms and Debates''</a> </li>
      <li><a href="https://ai-workshops.github.io/building-and-working-in-environments-for-embodied-ai-cvpr-2022/" target="_blank">CVPR 2022 Tutorial: Building and Working in Environments for Embodied AI </a> </li>

    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="pubs" href="#pubs">Selected Publications</a>
  </div>
  <div class="content publications">
    <ol>
      <a style="font-size: 20px;" href="./index.html#robotlearning" >[Robot Learning]</a>
      <a style="font-size: 20px;" href="./index.html#tactile">[Tactile Sensing]</a>
      <a style="font-size: 20px;" href="./index.html#3dcv">[3D Vision]</a>
      &nbsp;&nbsp; &nbsp;&nbsp;      *: equivalent contribution, †: corresponding author
      <div class="text-error" >Please refer to&nbsp;<a href="https://scholar.google.com/citations?user=azTXasgAAAAJ" target="_blank">Google Scholar</a>&nbsp;for the full publication list.</div>
      <li>
        <div class="imgblock"><img src="publications/dexsim2real2.gif"></img></div>
        <span class="title" id="robotlearning" href="#robotlearning">DexSim2Real<sup>2</sup>:Building Explicit World Model for Precise Articulated Object Manipulations
        </span>
        <div class="info text-success italic" style="color: grey"> Taoran Jiang*, Liqian Ma*, Yixuan Guan, Jiaojiao Meng, Weihang Chen, Zecui Zeng, Lusong Li, Dan Wu, Jing Xu, <span style="font-weight:bold">Rui Chen†</span></div>
        <div class="info"><span class="label label-info">arXiv</span> <span class="label label-info">robot learning</span>
        </div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/2409.08750" target="_blank">[Paper]</a>
          <a class="" href="https://jiangtaoran.github.io/dexsim2real2_website/" target="_blank">[Project]</a>
          <a href="bibs/dexsim2real2.bib">[BibTex]</a>
        </div>
        <div class="info">We present DexSim2Real<sup>2</sup>, a novel robot learning framework for goal-conditioned articulated object manipulation using both two-finger grippers and multi-finger dexterous hands.
          The key of our framework is constructing an explicit world model of unseen articulated objects through active one-step interactions. This explicit world model enables sampling-based model predictive control to plan trajectories achieving different manipulation goals without needing human demonstrations or reinforcement learning. For dexterous multi-finger manipulation, we propose to utilize eigengrasp to reduce the high-dimensional action space, enabling more efficient trajectory searching. The robust generalizability of the explicit world model also enables advanced manipulation strategies, such as manipulating with different tools.
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="publications/maniskill3_tall.jpg"></img></div>
        <span class="title"> ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI
        </span>
        <div class="info text-success italic" style="color: grey"> Stone Tao, Fanbo Xiang, Arth Shukla, Yuzhe Qin, Xander Hinrichsen, Xiaodi Yuan, Chen Bao, Xinsong Lin, Yulin Liu, Tse-kai Chan, Yuan Gao, Xuanlin Li, Tongzhou Mu, Nan Xiao, Arnav Gurha, Zhiao Huang, Roberto Calandra, <span style="font-weight:bold">Rui Chen</span>, Shan Luo, Hao Su</div>
        <div class="info"><span class="label label-info">arXiv</span> <span class="label label-info">robot learning</span></div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/2410.00425" target="_blank">[Paper]</a>
          <a class="" href="https://maniskill.ai/" target="_blank">[Project]</a>
          <a class="" href="https://github.com/haosulab/ManiSkill" target="_blank">[Code]</a>
          <a href="bibs/maniskill3.bib">[BibTex]</a>
        </div>
        <div class="info">ManiSkill3 is an open-source, GPU-parallelized robotics simulator focusing on generalizable manipulation with contact-rich physics. It supports GPU parallelized simulation+rendering, heterogeneous simulation, and more via a simple object oriented API. ManiSkill3’s high-speed simulation and efficient rendering (30,000+ FPS), performs 10-1000x faster and uses 2-3x less GPU memory than competitors. The platform covers 12 diverse task domains including humanoids, mobile manipulation, and drawing. It provides realistic scenes and millions of demonstration frames. ManiSkill3 also includes comprehensive baselines for RL and learning-from-demonstrations algorithms.
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="publications/maniskill2.jpg"></img></div>
        <span class="title"> ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills
        </span>
        <div class="info text-success italic" style="color: grey">Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, Xiaodi Yuan, Pengwei Xie, Zhiao Huang,<span style="font-weight:bold">Rui Chen</span>, Hao Su</div>
        <div class="info"><span class="label label-info">ICLR 2023</span> <span class="label label-info">robot learning</span></div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/2302.04659" target="_blank">[Paper]</a>
          <a class="" href="https://maniskill2.github.io/" target="_blank">[Project]</a>
          <a class="" href="https://github.com/haosulab/ManiSkill2" target="_blank">[Code]</a>
          <a href="bibs/maniskill2.bib">[BibTex]</a>
        </div>
        <div class="info">ManiSkill2 is a unified benchmark for learning generalizable robotic manipulation skills powered by SAPIEN. It features 20 out-of-box task families with 2000+ diverse object models and 4M+ demonstration frames. Moreover, it empowers fast visual input learning algorithms so that a CNN-based policy can collect samples at about 2000 FPS with 1 GPU and 16 processes on a workstation. The benchmark can be used to study a wide range of algorithms: 2D & 3D vision-based reinforcement learning, imitation learning, sense-plan-act, etc.
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="publications/sim2real2.png"></img></div>
        <span class="title"> Sim2Real<sup>2</sup>: Actively Building Explicit Physics Model for Precise Articulated Object Manipulation
        </span>
        <div class="info text-success italic" style="color: grey">Liqian Ma, Jiaojiao Meng, Shuntao Liu, Weihang Chen, Jing Xu†, <span style="font-weight:bold">Rui Chen†</span>
        </div>
        <div class="info"><span class="label label-info">ICRA 2023</span> <span class="label label-info">robot learning</span></div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/2302.10693" target="_blank">[Paper]</a>
          <a class="" href="https://ttimelord.github.io/Sim2Real2-site/" target="_blank">[Project]</a>
          <a class="" href="https://github.com/TTimelord/Sim2Real2" target="_blank">[Code]</a>
          <a class="" href="https://youtu.be/Xj2N2Hy38P4" target="_blank">[Video]</a>
          <a href="bibs/sim2real2.bib">[BibTex]</a>
        </div>
        <div class="info">We present Sim2Real<sup>2</sup> to enable the robot to manipulate an unseen articulated object to the desired state precisely in the real world with no human demonstrations. We leverage recent advances in physics simulation and learning-based perception to build the interactive explicit physics model of the object and use it to plan a long-horizon manipulation trajectory to accomplish the task.
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="publications/Part3DRL.gif"></img></div>
        <span class="title"> Part-Guided 3D RL for Sim2Real Articulated Object Manipulation
        </span>
        <div class="info text-success italic" style="color: grey"> Pengwei Xie*,&nbsp;<span style="font-weight:bold">Rui Chen*</span>, Siang Chen*, Yuzhe Qin, Fanbo Xiang, Tianyu Sun, Jing Xu, Guijin Wang†, Hao Su
        </div>
        <div class="info"><span class="label label-info">RAL 2023</span> <span class="label label-info">robot learning</span></div>
        <div class="stuff">
          <a class="" href="https://ieeexplore.ieee.org/abstract/document/10242361" target="_blank">[IEEE Xplore]</a>
          <a class="" href="https://github.com/THU-VCLab/Part-Guided-3D-RL-for-Sim2Real-Articulated-Object-Manipulation" target="_blank">[Code]</a>
          <a href="https://www.youtube.com/watch?v=b8KvOjlGNJs">[Video]</a>
          <a href="bibs/part3drl.bib">[BibTex]</a>
        </div>
        <div class="info">We propose a novel part-guided 3D RL framework, which can learn to manipulate articulated objects without demonstrations. We combine the strengths of 2D segmentation and 3D RL to improve the efficiency of RL policy training. To improve the stability of the policy on real robots, we design a Frame-consistent Uncertainty-aware Sampling (FUS) strategy to get a condensed and hierarchical 3D representation. In addition, a single versatile RL policy can be trained on multiple articulated object manipulation tasks simultaneously in simulation and shows great generalizability to novel categories and instances.
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="publications/s4g.webp"></img></div>
        <span class="title"> S4G: Amodal Single-view Single-Shot SE(3) Grasp Detection in Cluttered Scenes
        </span>
        <div class="info text-success italic" style="color: grey"> Yuzhe Qin*,<span style="font-weight:bold">Rui Chen*</span>, Hao Zhu, Meng Song, Jing Xu, Hao Su
        </div>
        <div class="info"><span class="label label-info">CoRL 2019</span> <span class="label label-info">robot learning</span></div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/1910.142181" target="_blank">[Paper]</a>
          <a class="" href="https://sites.google.com/view/s4ggrapsing" target="_blank">[Project]</a>
          <a href="https://youtu.be/Xlq4nw2AGcY">[Video]</a>
          <a href="bibs/s4g.bib">[BibTex]</a>
        </div>
        <div class="info">We studied the problem of 6-DoF grasping by a parallel gripper in a cluttered scene captured using a commodity depth sensor from a single view point. Our learning based approach trained in a synthetic scene can work well in real-world scenarios, with improved speed and success rate compared with state-of-the-arts.
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="publications/thintact.png"></img></div>
        <span class="title" id="tactile" href="#tactile">ThinTact: Thin Vision-Based Tactile Sensor by Lensless Imaging
        </span>
        <div class="info text-success italic" style="color: grey">Jing Xu*, Weihang Chen*, Hongyu Qian, Dan Wu, <span style="font-weight:bold">Rui Chen†</span>
        </div>
        <div class="info"><span class="label label-info">TRO 2025</span> <span class="label label-info">tactile sensing</span></div>
        <div class="stuff">
          <a class="" href="https://ieeexplore.ieee.org/document/10842357" target="_blank">[IEEE Xplore]</a>
          <a class="" href="https://arxiv.org/pdf/2501.09273" target="_blank">[PDF]</a>
          <a href="https://youtu.be/YrOO9BDMAHo?si=kMgEd47OvDmAxW3u">[Video]</a>
          <a href="bibs/thintact.bib">[BibTex]</a>
        </div>
        <div class="info">We propose ThinTact, a thin vision-based tactile sensor with a thickness of less than 10 mm. To overcome the thickness constraint of the lens system, we utilize the amplitude-mask-based lensless imaging technique to translate the contact information into CMOS signals. We first reconstruct a clear image from the CMOS signal, and then compute the contact geometry and marker displacements. The high sensitivity and thin profile of ThinTact enables many applications, including texture recognition, delicate object grasping and object manipulation.
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="publications/tro_tactile_sim2real.gif"></img></div>
        <span class="title"> General-Purpose Sim2Real Protocol for Learning Contact-Rich Manipulation With Marker-Based Visuotactile Sensors
        </span>
        <div class="info text-success italic" style="color: grey">Weihang Chen*, Jing Xu*†, Fanbo Xiang, Xiaodi Yuan, Hao Su†,<span style="font-weight:bold">Rui Chen†</span>
        </div>
        <div class="info"><span class="label label-info">TRO 2024</span> <span class="label label-info">tactile sensing</span></div>
        <div class="stuff">
          <a class="" href="https://ieeexplore.ieee.org/abstract/document/10388459" target="_blank">[IEEE Xplore (Open Access)]</a>
          <a class="" href="https://callmeray.github.io/tactile_sim2real_page/" target="_blank">[Project]</a>
          <a href="https://ieeexplore.ieee.org/abstract/document/10388459/media#media">[Video]</a>
          <a href="bibs/tactilesim2real.bib">[BibTex]</a>
        </div>
        <div class="info">We employ an FEM-based physics simulator that can simulate the sensor deformation accurately and stably for arbitrary geometries. We further propose a novel tactile feature extraction network that directly processes the set of pixel coordinates of tactile sensor markers and a self-supervised pretraining strategy to improve the efficiency and generalizability of RL policies. We conduct extensive Sim2Real experiments on the peg-in-hole task to validate the effectiveness of our method. And we further show its generalizability on additional tasks including plug adjustment and lock opening.
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="publications/Transtouch.gif"></img></div>
        <span class="title">Transtouch: Learning Transparent Object Depth Sensing Through Sparse Touches
        </span>
        <div class="info text-success italic" style="color: grey">Liuyu Bian*, Pengyang Shi*, Weihang Chen, Jing Xu, Li Yi†, <span style="font-weight:bold">Rui Chen†</span>
        </div>
        <div class="info"><span class="label label-info">IROS 2023</span> <span class="label label-info">tactile sensing</span></div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/2309.09427" target="_blank">[Paper]</a>
          <a class="" href="https://github.com/ritsu-a/transtouch" target="_blank">[Code]</a>
          <a href="https://youtu.be/WbkK1TZfy3M">[Video]</a>
          <a href="bibs/transtouch.bib">[BibTex]</a>
        </div>
        <div class="info">We propose a method to finetune a stereo network with sparse depth labels automatically collected using a probing system with tactile feedback. We present a novel utility function to evaluate the benefit of touches. By approximating and optimizing the utility function, we can optimize the probing locations given a fixed touching budget to better improve the network's performance on real objects. We further combine tactile depth supervision with a confidence-based regularization to prevent over-fitting during finetuning.
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="publications/bidirection.webp"></img></div>
        <span class="title"> Bidirectional Sim-to-Real Transfer for GelSight Tactile Sensors with CycleGAN
        </span>
        <div class="info text-success italic" style="color: grey"> Weihang Chen*, Yuan Xu*, Zhenyang Chen*, Peiyu Zeng, Renjun Dang, <span style="font-weight:bold">Rui Chen</span>, Jing Xu†
        </div>
        <div class="info"><span class="label label-info">RAL 2022</span> <span class="label label-info">tactile sensing</span></div>
        <div class="stuff">
          <a class="" href="https://ieeexplore.ieee.org/document/9756938" target="_blank">[IEEE Xplore]</a>
          <a class="" href="https://github.com/RVSATHU/GelSight-Sim2Real" target="_blank">[Code]</a>
          <a href="bibs/tactilegan.bib">[BibTex]</a>
        </div>
        <div class="info">We propose to narrow the gap between simulation and real world using CycleGAN. Due to the bidirectional generators of CycleGAN, the proposed method can not only generate more realistic simulated tactile images, but also improve the deformation measurement accuracy of real sensors by transferring them to simulation domain.
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="publications/activezero++.jpg"></img></div>
        <span class="title" id="3dcv" href="#3dcv"> ActiveZero++: Mixed Domain Learning Stereo and Confidence-based Depth Completion with Zero Annotation
        </span>
        <div class="info text-success italic" style="color: grey"><span style="font-weight:bold">Rui Chen</span>, Isabella Liu, Edward Yang, Jianyu Tao, Xiaoshuai Zhang, Qing Ran, Zhu Liu, Jing Xu†, Hao Su
        </div>
        <div class="info"><span class="label label-info">TPAMI 2023</span> <span class="label label-info">3d vision</span></div>
        <div class="stuff">
          <a class="" href="https://ieeexplore.ieee.org/abstract/document/10219021" target="_blank">[IEEE Xplore]</a>
          <a class="" href="https://github.com/jaycions/activezero2_official" target="_blank">[Code]</a>
          <a href="bibs/activezero2.bib">[BibTex]</a>
        </div>
        <div class="info">We propose a new framework, ActiveZero++, which is a mixed domain learning solution for active stereovision systems that requires no real world depth annotation. In the simulation domain, we use a combination of supervised disparity loss and self-supervised loss on a shape primitives dataset. By contrast, in the real domain, we only use self-supervised loss on a dataset that is out-of-distribution from either training simulation data or test real data. To improve the robustness and accuracy of our reprojection loss in hard-to-perceive regions, our method introduces a novel self-supervised loss called temporal IR reprojection. Further, we propose the confidence-based depth completion module, which uses the confidence from the stereo network to identify and improve erroneous areas in depth prediction through depth-normal consistency.
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="publications/alignment.gif"></img></div>
        <span class="title"> Close the Optical Sensing Domain Gap by Physics-Grounded Active Stereo Sensor Simulation
        </span>
        <div class="info text-success italic" style="color: grey"> Xiaoshuai Zhang*, <span style="font-weight:bold">Rui Chen*</span>, Ang Li**, Fanbo Xiang**, Yuzhe Qin**, Jiayuan Gu**, Zhan Ling**, Minghua Liu**, Peiyu Zeng**, Songfang Han***, Zhiao Huang***, Tongzhou Mu***, Jing Xu†, Hao Su†
        </div>
        <div class="info"><span class="label label-info">TRO 2023</span> <span class="label label-info">3d vision</span></div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/pdf/2201.11924.pdf" target="_blank">[Paper]</a>
          <a class="" href="https://angli66.github.io/active-sensor-sim/" target="_blank">[Project]</a>
          <a href="https://sapien.ucsd.edu/docs/latest/tutorial/rendering/depth_sensor.html">[Document]</a>
          <a href="bibs/realisticdepth.bib">[BibTex]</a>
        </div>
        <div class="info">SAPIEN Realistic depth lowers the sim-to-real gap of simulated depth and real active stereovision depth sensors, by designing a fully physics-grounded pipeline. Perception and RL methods trained in simulation can transfer well to the real world without any fine-tuning. It can also estimate the algorithm performance in the real world, largely reducing human effort of algorithm evaluation.
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="publications/pointmvsnet.webp"></img></div>
        <span class="title"> Visibility-Aware Point-Based Multi-View Stereo Network
        </span>
        <div class="info text-success italic" style="color: grey"> <span style="font-weight:bold">Rui Chen</span>, Songfang Han, Jing Xu†, Hao Su
        </div>
        <div class="info"><span class="label label-info">TPAMI 2020</span> <span class="label label-info">3d vision</span></div>
        <div class="stuff">
          <a class="" href="https://ieeexplore.ieee.org/document/9076298" target="_blank">[IEEE Xplore (Open Access)]</a>
          <a class="" href="https://github.com/callmeray/PointMVSNet/tree/va_point_mvsnet" target="_blank">[Code]</a>
          <a href="bibs/vapointmvsnet.bib">[BibTex]</a>
        </div>
        <div class="info">This is an extension of our ICCV work (Point-based Multi-view Stereo Network). In this paper, we introduce visibility-aware multi-view feature aggregation modules to gather information from visible views only for better depth prediction accuracy.
        </div>


        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="publications/pointmvsnet.webp"></img></div>
        <span class="title"> Point-based Multi-View Stereo Network
        </span>
        <div class="info text-success italic" style="color: grey"> <span style="font-weight:bold">Rui Chen*</span>, Songfang Han*, Jing Xu, Hao Su
        </div>
        <div class="info"><span class="label label-info">ICCV 2019 (Oral)</span> <span class="label label-info">3d vision</span></div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/1908.04422?context=cs" target="_blank">[Paper]</a>
          <a class="" href="http://hansf.me/projects/PMVSNet/" target="_blank">[Project]</a>
          <a class="" href="https://github.com/callmeray/PointMVSNet/" target="_blank">[Code]</a>
          <a href="https://www.youtube.com/watch?v=eSBFOD5rDsU">[Video]</a>
          <a href="https://conftube.com/video/my3jocjpD0U?tocitem=59">[Presentation]</a>
          <a href="bibs/pointmvsnet.bib">[BibTex]</a>
        </div>
        <div class="info"> We introduce Point-MVSNet, a novel point-based deep framework for multi-view stereo (MVS). Distinct from existing cost volume approaches, our method directly processes the target scene as point clouds. More specifically, our method predicts the depth in a coarse-to-fine manner. We first generate a coarse depth map, convert it into a point cloud and refine the point cloud iteratively by estimating the residual between the depth of the current iteration and that of the ground truth. Our network leverages 3D geometry priors and 2D texture information jointly and effectively by fusing them into a feature-augmented point cloud, and processes the point cloud to estimate the 3D flow for each point. This point-based architecture allows higher accuracy, more computational efficiency and more flexibility than cost-volume-based counterparts.
        </div>
        <div style="clear:both"></div>
      </li>


    </ol>
  </div>

  <div class="title">
    <a class="title_link" id="awards" href="#awards">Awards</a>
  </div>


  <div class="content">
    <ul>
      <li>北京市科协青年人才托举工程 <div class="float-right">2024</div></li>
      <li>
        博士后创新人才支持计划 <div class="float-right">2021</div>
      </li>
      <li>清华大学''水木学者''计划 <div class="float-right">2020</div></li>
    </ul>
  </div>



</div>

</body>
</html>
